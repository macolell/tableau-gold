{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "oppertunity = pd.read_csv('/Users/michaelcolellajensen/Desktop/Tableau/case_oppertunities.csv')\n",
    "case_text = pd.read_csv('/Users/michaelcolellajensen/Desktop/Tableau/support_cases_text_fact.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'STATUS', 'PRIORITY', 'IMPACT', 'ORIGIN', 'PRODUCT',\n",
      "       'CATEGORY', 'SUBCATEGORY', 'CREATED_DATE', 'CLOSED_DATE',\n",
      "       'CASE_CLOSED_REASON', 'RECORDTYPE', 'CASE_NUMBER', 'ACCOUNTKEY',\n",
      "       'ACCOUNTTYPE', 'ACCOUNTTYPEDETAIL', 'EMPLOYEECOUNT', 'THEATER',\n",
      "       'SUBREGION', 'SEGMENT', 'SUBSEGMENT', 'Tenure', 'Zero.Revenue.Flag',\n",
      "       'Einstein Analytics', 'Role Based - Online', 'Server', 'Saas',\n",
      "       'Role Based - On Prem', 'Services', 'Desktop', 'Role Based - NonProd',\n",
      "       'Other', 'CLOSEDATE', 'ISWON', 'ISPARTNERRELATED'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(oppertunity.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54546, 40)\n"
     ]
    }
   ],
   "source": [
    "sales = oppertunity[oppertunity['ISWON'] == 1]\n",
    "sales = sales.merge(case_text,\n",
    "                          how='left',\n",
    "                          on='CASE_NUMBER')\n",
    "print(sales.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5862574707586258\n",
      "0.1748249184174825\n",
      "0.021486451802148646\n",
      "0.05212114545521211\n",
      "0.15462178711546218\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "sales['Einstein Analytics'] = np.where(sales['Einstein Analytics'] >0, 1, 0)\n",
    "sales['Role Based - Online & Saas'] = np.where((sales['Role Based - Online'] >0) |\n",
    "                                        (sales['Saas'] > 0), 1, 0)\n",
    "sales['Server'] = np.where(sales['Server'] >0, 1, 0)\n",
    "sales['Role Based - On Prem'] = np.where((sales['Role Based - On Prem'] >0) &\n",
    "                                         (sales['Einstein Analytics'] == 0) &\n",
    "                                         (sales['Server'] == 0) &\n",
    "                                         (sales['Saas'] == 0) &\n",
    "                                         (sales['Role Based - Online'] == 0) &\n",
    "                                         (sales['Services'] == 0) &\n",
    "                                         (sales['Desktop'] == 0) &\n",
    "                                         (sales['Other'] == 0) &\n",
    "                                         (sales['Role Based - NonProd'] == 0), 1, 0)\n",
    "sales['Services'] = np.where(sales['Services'] >0, 1, 0)\n",
    "sales['Other'] = np.where((sales['Other'] >0) |\n",
    "                          (sales['Role Based - NonProd'] >0) |\n",
    "                          (sales['Desktop'] >0), 1, 0)\n",
    "\n",
    "print(sales['Role Based - On Prem'].sum() / len(sales))\n",
    "print(sales['Role Based - Online & Saas'].sum() / len(sales))\n",
    "print(sales['Einstein Analytics'].sum() / len(sales))\n",
    "print(sales['Server'].sum() / len(sales))\n",
    "print(sales['Other'].sum() / len(sales))\n",
    "\n",
    "labels = sales[['Role Based - On Prem',\n",
    "                'Einstein Analytics', \n",
    "                'Role Based - Online & Saas', \n",
    "                'Server',\n",
    "                'Other']]\n",
    "num_classes = len(labels.columns)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'STATUS', 'PRIORITY', 'IMPACT', 'ORIGIN', 'PRODUCT',\n",
      "       'CATEGORY', 'SUBCATEGORY', 'CREATED_DATE', 'CLOSED_DATE',\n",
      "       'CASE_CLOSED_REASON', 'RECORDTYPE', 'CASE_NUMBER', 'ACCOUNTKEY',\n",
      "       'ACCOUNTTYPE', 'ACCOUNTTYPEDETAIL', 'EMPLOYEECOUNT', 'THEATER',\n",
      "       'SUBREGION', 'SEGMENT', 'SUBSEGMENT', 'Tenure', 'Zero.Revenue.Flag',\n",
      "       'Einstein Analytics', 'Role Based - Online', 'Server', 'Saas',\n",
      "       'Role Based - On Prem', 'Services', 'Desktop', 'Role Based - NonProd',\n",
      "       'Other', 'CLOSEDATE', 'ISWON', 'ISPARTNERRELATED', 'completetion_time',\n",
      "       'count'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t_/rcfcs8g56jn7trwnsvmdyh_r0000gn/T/ipykernel_53076/2069459914.py:13: FutureWarning: casting timedelta64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
      "  oppertunity['completetion_time'] = oppertunity['completetion_time'].astype('int')\n"
     ]
    }
   ],
   "source": [
    "cases = pd.read_csv('/Users/michaelcolellajensen/Desktop/Tableau/support_cases_fact.csv')\n",
    "oppertunity = pd.read_csv('/Users/michaelcolellajensen/Desktop/Tableau/case_oppertunities.csv')\n",
    "casetext = pd.read_csv('/Users/michaelcolellajensen/Desktop/Tableau/support_cases_text_fact.csv')\n",
    "cases_by_account = cases.groupby('ACCOUNTKEY')['CASE_NUMBER'].agg(['count'])\n",
    "#print(cases_by_account.head)\n",
    "oppertunity = oppertunity[oppertunity['ISWON'] == 1]\n",
    "oppertunity['completetion_time'] = pd.to_datetime(oppertunity['CLOSED_DATE']) - pd.to_datetime(oppertunity['CREATED_DATE'])\n",
    "\n",
    "oppertunity = oppertunity.merge(cases_by_account,\n",
    "                                on = 'ACCOUNTKEY',\n",
    "                                how='left')\n",
    "print(oppertunity.columns)\n",
    "oppertunity['completetion_time'] = oppertunity['completetion_time'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'SUBJECT', 'DESCRIPTION', 'ISSUE_SUMMARY',\n",
      "       'CLOSED_CASE_NOTES', 'CASE_NUMBER'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(casetext.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t_/rcfcs8g56jn7trwnsvmdyh_r0000gn/T/ipykernel_53076/464594555.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  oppertunity_text['text'] = oppertunity_text['DESCRIPTION']\\\n",
      "/var/folders/t_/rcfcs8g56jn7trwnsvmdyh_r0000gn/T/ipykernel_53076/464594555.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  .str.replace('[^a-zA-Z]', ' ').str.lower() + oppertunity_text['ISSUE_SUMMARY']\\\n",
      "/var/folders/t_/rcfcs8g56jn7trwnsvmdyh_r0000gn/T/ipykernel_53076/464594555.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  .str.replace('[^a-zA-Z]', ' ').str.lower() + oppertunity_text['DESCRIPTION']\\\n",
      "/Users/michaelcolellajensen/Library/Python/3.8/lib/python/site-packages/pandas/core/frame.py:5176: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "oppertunity_text = oppertunity.merge(casetext,\n",
    "                                     how='left',\n",
    "                                     on = 'CASE_NUMBER')\n",
    "oppertunity_text['text'] = oppertunity_text['DESCRIPTION']\\\n",
    "    .str.replace('[^a-zA-Z]', ' ').str.lower() + oppertunity_text['ISSUE_SUMMARY']\\\n",
    "        .str.replace('[^a-zA-Z]', ' ').str.lower() + oppertunity_text['DESCRIPTION']\\\n",
    "            .str.replace('[^a-zA-Z]', ' ').str.lower()\n",
    "text = oppertunity_text[['text']]\n",
    "text.fillna('None', inplace=True)\n",
    "tfidf = TfidfVectorizer(max_features= 1000,\n",
    "                        max_df=0.5,\n",
    "                        ngram_range=(1, 2),\n",
    "                        stop_words='english')\n",
    "text_tfidf = tfidf.fit_transform(text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(text_tfidf.toarray(), \n",
    "                       columns = tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54546, 5)\n",
      "(54546, 1150)\n"
     ]
    }
   ],
   "source": [
    "case_categories = oppertunity[['STATUS','PRIORITY', 'IMPACT', 'ORIGIN', 'PRODUCT',\n",
    "                                    'ACCOUNTTYPE', 'ACCOUNTTYPEDETAIL','THEATER',\n",
    "                                    'SUBREGION', 'SEGMENT', 'SUBSEGMENT']]\n",
    "descrete = oppertunity[['Tenure', 'completetion_time', 'count', 'EMPLOYEECOUNT']]\n",
    "case_one_hot_encode = pd.get_dummies(case_categories)\n",
    "case_context_features = pd.concat([case_one_hot_encode, \n",
    "                                   descrete],\n",
    "                                   axis=1)\n",
    "\n",
    "case_context_features['completetion_time'] = case_context_features['completetion_time'].astype('int')\n",
    "case_features = pd.concat([case_context_features, text_df],axis=1)\n",
    "print(labels.shape)\n",
    "print(case_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelcolellajensen/Library/Python/3.8/lib/python/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:40:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:41:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:41:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:43:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:43:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:44:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:44:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Overall Accuracy: 0.685194690914424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(case_context_features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=labels,\n",
    "                                                    random_state=123)    \n",
    "xgb_model = xgb.XGBClassifier(n_estimators = 770,\n",
    "                              eta = 0.105,\n",
    "                              max_depth = 8,\n",
    "                              max_delta_step = 4,\n",
    "                              min_child_weight = 0.435,\n",
    "                              min_sample_split = 0.225,\n",
    "                              n_jobs = 8)\n",
    "multiclass_model = OneVsRestClassifier(xgb_model)\n",
    "multiclass_model.fit(x_train, y_train)\n",
    "predictions = multiclass_model.predict(x_test)\n",
    "print('Overall Accuracy: ' + str(multiclass_model.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role Based - On Prem: 0.8537836147592245\n",
      "Einstein Analytics: 0.2627986348122867\n",
      "Role Based - Online & Saas: 0.6034410407049937\n",
      "Server: 0.2039381153305204\n",
      "Other: 0.2077798861480076\n"
     ]
    }
   ],
   "source": [
    "y_class0 = y_test['Role Based - On Prem'].values\n",
    "predictions0 = predictions[:,0]\n",
    "class0 = np.where((y_class0 == predictions0) &\n",
    "                   ((predictions0 > 0)|\n",
    "                    (y_class0 > 0)), 1, 0)\n",
    "class0_acc = class0.sum() / y_class0.sum()\n",
    "print('Role Based - On Prem: ' + str(class0_acc))\n",
    "\n",
    "y_class1 = y_test['Einstein Analytics'].values\n",
    "predictions1 = predictions[:,1]\n",
    "class1 = np.where((y_class1 == predictions1) &\n",
    "                   ((predictions1 > 0)|\n",
    "                    y_class1>0), 1, 0)\n",
    "class1_acc = class1.sum() / y_class1.sum()\n",
    "print('Einstein Analytics: ' + str(class1_acc))\n",
    "\n",
    "y_class2 = y_test['Role Based - Online & Saas'].values\n",
    "predictions2 = predictions[:,2]\n",
    "class2 = np.where((y_class2 == predictions2) &\n",
    "                   ((predictions2 > 0)|\n",
    "                    (y_class2 > 0)), 1, 0)\n",
    "class2_acc = class2.sum() / y_class2.sum()\n",
    "print('Role Based - Online & Saas: ' + str(class2_acc))\n",
    "\n",
    "y_class3 = y_test['Server'].values\n",
    "predictions3 = predictions[:,3]\n",
    "class3 = np.where((y_class3 == predictions3) &\n",
    "                   ((predictions3 > 0)|\n",
    "                    y_class3 > 0), 1, 0)\n",
    "class3_acc = class3.sum() / y_class3.sum()\n",
    "print('Server: ' + str(class3_acc))\n",
    "\n",
    "y_class4 = y_test['Other'].values\n",
    "predictions4 = predictions[:,4]\n",
    "class4 = np.where((y_class4 == predictions2) &\n",
    "                   ((predictions4 > 0)|\n",
    "                    (y_class4 > 0)), 1, 0)\n",
    "class4_acc = class4.sum() / y_class4.sum()\n",
    "print('Other: ' + str(class4_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelcolellajensen/Library/Python/3.8/lib/python/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:42:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:42:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:44:49] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:44:49] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:46:43] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:46:43] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:48:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:48:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:50:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"min_sample_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:50:36] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Overall Accuracy: 0.5497543447972428\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_test_text, y_train_text, y_test_text = train_test_split(text_tfidf,\n",
    "                                                                        labels,\n",
    "                                                                        test_size=0.25,\n",
    "                                                                        stratify=labels,\n",
    "                                                                        random_state=123)    \n",
    "xgb_text_model = xgb.XGBClassifier(n_estimators = 550,\n",
    "                                   eta = 0.05,\n",
    "                                   max_depth = 12,\n",
    "                                   max_delta_step = 4,\n",
    "                                   min_child_weight = 0.435,\n",
    "                                   min_sample_split = 0.125,\n",
    "                                   n_jobs = 8)\n",
    "text_multiclass = OneVsRestClassifier(xgb_text_model)\n",
    "text_multiclass.fit(x_train_text, y_train_text)\n",
    "text_predictions = text_multiclass.predict(x_test_text)\n",
    "print('Overall Accuracy: ' + str(text_multiclass.score(x_test_text, y_test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          score\n",
      "f844  66.383354\n",
      "f253  44.469250\n",
      "f86   13.678738\n",
      "f993  13.127463\n",
      "f449  10.584758\n",
      "f54    9.244251\n",
      "f852   9.110429\n",
      "f699   8.515123\n",
      "f372   7.805783\n",
      "f856   7.775749\n",
      "f102   7.620206\n",
      "f541   7.411538\n",
      "f995   7.237294\n",
      "f951   7.023158\n",
      "f29    6.949738\n",
      "f619   6.788813\n",
      "f959   6.761335\n",
      "f173   6.758111\n",
      "f763   6.738438\n",
      "f532   6.635305\n"
     ]
    }
   ],
   "source": [
    "dict = text_multiclass.estimators_[2].get_booster().get_score(importance_type='gain')\n",
    "keys = list(dict.keys())\n",
    "values = list(dict.values())\n",
    "feature_df = pd.DataFrame(data=values,\n",
    "                          index=keys,\n",
    "                          columns= ['score'],\n",
    "                          ).sort_values(by='score',\n",
    "                                        ascending=False)\n",
    "print(feature_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tableausoftware tabadmin\n",
      "https help\n",
      "sales\n",
      "tableausoftware\n"
     ]
    }
   ],
   "source": [
    "feature_words = tfidf.get_feature_names()\n",
    "print(feature_words[856])\n",
    "print(feature_words[372])\n",
    "print(feature_words[699])\n",
    "print(feature_words[852])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
